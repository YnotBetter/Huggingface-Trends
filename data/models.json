{
  "models": [
    {
      "id": "Qwen/Qwen2.5-7B-Instruct",
      "name": "Qwen2.5-7B-Instruct",
      "category": "llm",
      "params": "7B",
      "description": "Starkes multilinguales LLM von Alibaba. Exzellent für Chat, Coding und Reasoning.",
      "use_cases": ["Chat", "Coding", "Reasoning", "Multilingual"],
      "recommended": true,
      "python_package": "transformers",
      "python_code": "from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"Qwen/Qwen2.5-7B-Instruct\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n\nmessages = [{\"role\": \"user\", \"content\": \"Erkläre mir Quantencomputing.\"}]\ntext = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\ninputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n\noutputs = model.generate(**inputs, max_new_tokens=512)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))",
      "install_cmd": "pip install transformers torch accelerate"
    },
    {
      "id": "Qwen/Qwen2.5-14B-Instruct",
      "name": "Qwen2.5-14B-Instruct",
      "category": "llm",
      "params": "14B",
      "description": "Größere Variante von Qwen2.5 mit verbesserter Reasoning-Fähigkeit.",
      "use_cases": ["Chat", "Coding", "Reasoning", "Analysis"],
      "recommended": true,
      "python_package": "transformers",
      "python_code": "from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"Qwen/Qwen2.5-14B-Instruct\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=\"auto\")\n\nmessages = [{\"role\": \"user\", \"content\": \"Schreibe eine Python-Funktion für Fibonacci.\"}]\ntext = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\ninputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n\noutputs = model.generate(**inputs, max_new_tokens=512)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))",
      "install_cmd": "pip install transformers torch accelerate"
    },
    {
      "id": "meta-llama/Llama-3.1-8B-Instruct",
      "name": "Llama 3.1 8B Instruct",
      "category": "llm",
      "params": "8B",
      "description": "Metas neuestes Open-Source LLM. Stark in Englisch, gut für allgemeine Aufgaben.",
      "use_cases": ["Chat", "Coding", "Instruction Following"],
      "recommended": true,
      "python_package": "transformers",
      "python_code": "from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"meta-llama/Llama-3.1-8B-Instruct\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=\"auto\")\n\nmessages = [{\"role\": \"user\", \"content\": \"What is machine learning?\"}]\ninputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(model.device)\n\noutputs = model.generate(inputs, max_new_tokens=256)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))",
      "install_cmd": "pip install transformers torch accelerate"
    },
    {
      "id": "google/gemma-2-9b-it",
      "name": "Gemma 2 9B Instruct",
      "category": "llm",
      "params": "9B",
      "description": "Googles effizientes Open-Source LLM. Gut balanciert zwischen Größe und Leistung.",
      "use_cases": ["Chat", "Reasoning", "Instruction Following"],
      "recommended": true,
      "python_package": "transformers",
      "python_code": "from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"google/gemma-2-9b-it\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=\"auto\")\n\nmessages = [{\"role\": \"user\", \"content\": \"Explain neural networks simply.\"}]\ninputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(model.device)\n\noutputs = model.generate(inputs, max_new_tokens=256)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))",
      "install_cmd": "pip install transformers torch accelerate"
    },
    {
      "id": "deepseek-ai/DeepSeek-V2-Lite-Chat",
      "name": "DeepSeek V2 Lite Chat",
      "category": "llm",
      "params": "16B",
      "description": "Effizientes MoE-Modell von DeepSeek. Sehr gutes Preis-Leistungs-Verhältnis.",
      "use_cases": ["Chat", "Coding", "Reasoning"],
      "recommended": true,
      "python_package": "transformers",
      "python_code": "from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"deepseek-ai/DeepSeek-V2-Lite-Chat\"\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", trust_remote_code=True)\n\nmessages = [{\"role\": \"user\", \"content\": \"Write a sorting algorithm.\"}]\ninputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(model.device)\n\noutputs = model.generate(inputs, max_new_tokens=512)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))",
      "install_cmd": "pip install transformers torch accelerate"
    },
    {
      "id": "BAAI/bge-large-en-v1.5",
      "name": "BGE Large English v1.5",
      "category": "embedding",
      "params": "335M",
      "description": "Eines der besten Open-Source Embedding-Modelle für RAG. Englisch-fokussiert.",
      "use_cases": ["RAG", "Semantic Search", "Document Retrieval"],
      "recommended": true,
      "python_package": "sentence-transformers",
      "python_code": "from sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer('BAAI/bge-large-en-v1.5')\n\n# Dokumente einbetten\ndocuments = [\n    \"Machine learning is a subset of AI.\",\n    \"Python is a programming language.\",\n    \"Neural networks mimic brain structure.\"\n]\ndoc_embeddings = model.encode(documents, normalize_embeddings=True)\n\n# Query einbetten (mit Instruction Prefix für BGE)\nquery = \"What is ML?\"\nquery_embedding = model.encode(f\"Represent this sentence for searching relevant passages: {query}\", normalize_embeddings=True)\n\n# Ähnlichkeit berechnen\nimport numpy as np\nsimilarities = np.dot(doc_embeddings, query_embedding)\nprint(f\"Most similar: {documents[np.argmax(similarities)]}\")",
      "install_cmd": "pip install sentence-transformers"
    },
    {
      "id": "BAAI/bge-m3",
      "name": "BGE M3",
      "category": "embedding",
      "params": "568M",
      "description": "Multilinguales Embedding-Modell. Unterstützt über 100 Sprachen für RAG.",
      "use_cases": ["RAG", "Multilingual Search", "Cross-lingual Retrieval"],
      "recommended": true,
      "python_package": "sentence-transformers",
      "python_code": "from sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer('BAAI/bge-m3')\n\n# Multilingual documents\ndocuments = [\n    \"Künstliche Intelligenz verändert die Welt.\",\n    \"L'intelligence artificielle change le monde.\",\n    \"Artificial intelligence is changing the world.\"\n]\n\nembeddings = model.encode(documents, normalize_embeddings=True)\n\n# Query in einer anderen Sprache\nquery = \"AI impact on society\"\nquery_emb = model.encode(query, normalize_embeddings=True)\n\nimport numpy as np\nsimilarities = np.dot(embeddings, query_emb)\nfor doc, sim in zip(documents, similarities):\n    print(f\"{sim:.3f}: {doc}\")",
      "install_cmd": "pip install sentence-transformers"
    },
    {
      "id": "intfloat/multilingual-e5-large-instruct",
      "name": "Multilingual E5 Large Instruct",
      "category": "embedding",
      "params": "560M",
      "description": "Instruction-tuned multilinguales Embedding von Microsoft. Sehr flexibel.",
      "use_cases": ["RAG", "Semantic Search", "Classification"],
      "recommended": true,
      "python_package": "sentence-transformers",
      "python_code": "from sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer('intfloat/multilingual-e5-large-instruct')\n\n# Mit Task-Instruction\ndef get_embedding(text, task=\"retrieval.query\"):\n    instruction = f\"Instruct: Retrieve relevant documents\\nQuery: {text}\"\n    return model.encode(instruction, normalize_embeddings=True)\n\ndocs = [\"Python ist eine Programmiersprache.\", \"JavaScript läuft im Browser.\"]\nquery_emb = get_embedding(\"Welche Sprache für Web?\")\ndoc_embs = model.encode(docs, normalize_embeddings=True)\n\nimport numpy as np\nprint(docs[np.argmax(np.dot(doc_embs, query_emb))])",
      "install_cmd": "pip install sentence-transformers"
    },
    {
      "id": "stepfun-ai/GOT-OCR2_0",
      "name": "GOT-OCR 2.0",
      "category": "ocr",
      "params": "580M",
      "description": "State-of-the-art OCR-Modell. Unterstützt Dokumente, Formeln, Tabellen und mehr.",
      "use_cases": ["Document OCR", "Formula Recognition", "Table Extraction"],
      "recommended": true,
      "python_package": "transformers",
      "python_code": "from transformers import AutoModel, AutoTokenizer\nfrom PIL import Image\n\nmodel_name = \"stepfun-ai/GOT-OCR2_0\"\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\nmodel = AutoModel.from_pretrained(model_name, trust_remote_code=True, device_map=\"auto\")\n\n# OCR auf einem Bild\nimage_path = \"document.png\"\nresult = model.chat(tokenizer, image_path, ocr_type='ocr')\nprint(result)\n\n# Für formatierte Ausgabe (Markdown)\nresult_formatted = model.chat(tokenizer, image_path, ocr_type='format')\nprint(result_formatted)",
      "install_cmd": "pip install transformers torch accelerate tiktoken verovio"
    },
    {
      "id": "microsoft/trocr-large-printed",
      "name": "TrOCR Large Printed",
      "category": "ocr",
      "params": "558M",
      "description": "Microsofts Transformer-OCR für gedruckten Text. Sehr zuverlässig.",
      "use_cases": ["Printed Text OCR", "Document Digitization"],
      "recommended": true,
      "python_package": "transformers",
      "python_code": "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\nfrom PIL import Image\nimport requests\n\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-large-printed')\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-printed')\n\n# Bild laden\nimage = Image.open(\"text_image.png\").convert(\"RGB\")\n\n# OCR durchführen\npixel_values = processor(images=image, return_tensors=\"pt\").pixel_values\ngenerated_ids = model.generate(pixel_values)\ntext = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\nprint(text)",
      "install_cmd": "pip install transformers torch pillow"
    },
    {
      "id": "ds4sd/docling-ibm-granite-timesformer-vision",
      "name": "Docling IBM Granite Vision",
      "category": "ocr",
      "params": "316M",
      "description": "IBMs Docling-Modell für Dokumentverarbeitung. Gut für komplexe Layouts.",
      "use_cases": ["Document Understanding", "Layout Analysis", "PDF Processing"],
      "recommended": false,
      "python_package": "docling",
      "python_code": "from docling.document_converter import DocumentConverter\n\nconverter = DocumentConverter()\nresult = converter.convert(\"document.pdf\")\n\n# Als Markdown exportieren\nmarkdown = result.document.export_to_markdown()\nprint(markdown)\n\n# Oder als strukturierte Daten\nfor element in result.document.texts:\n    print(f\"{element.label}: {element.text}\")",
      "install_cmd": "pip install docling"
    },
    {
      "id": "coqui/XTTS-v2",
      "name": "XTTS v2",
      "category": "tts",
      "params": "467M",
      "description": "Multilinguales TTS mit Voice Cloning. Unterstützt 17 Sprachen inkl. Deutsch.",
      "use_cases": ["Text-to-Speech", "Voice Cloning", "Multilingual TTS"],
      "recommended": true,
      "python_package": "TTS",
      "python_code": "from TTS.api import TTS\n\n# XTTS v2 laden\ntts = TTS(model_name=\"tts_models/multilingual/multi-dataset/xtts_v2\", gpu=True)\n\n# Text zu Sprache (Deutsch)\ntts.tts_to_file(\n    text=\"Hallo, das ist ein Test der Sprachsynthese.\",\n    file_path=\"output.wav\",\n    language=\"de\"\n)\n\n# Mit Voice Cloning\ntts.tts_to_file(\n    text=\"Dies ist meine geklonte Stimme.\",\n    file_path=\"cloned_output.wav\",\n    speaker_wav=\"reference_voice.wav\",\n    language=\"de\"\n)",
      "install_cmd": "pip install TTS"
    },
    {
      "id": "suno/bark",
      "name": "Bark",
      "category": "tts",
      "params": "1B",
      "description": "Sunos TTS mit Emotionen, Musik und Sound-Effekten. Sehr expressiv.",
      "use_cases": ["Expressive TTS", "Sound Effects", "Music Generation"],
      "recommended": true,
      "python_package": "transformers",
      "python_code": "from transformers import AutoProcessor, BarkModel\nimport scipy.io.wavfile as wav\nimport torch\n\nprocessor = AutoProcessor.from_pretrained(\"suno/bark\")\nmodel = BarkModel.from_pretrained(\"suno/bark\", torch_dtype=torch.float16).to(\"cuda\")\n\n# Text mit Emotionen\ntext = \"[laughs] Hallo! Das ist wirklich lustig. [music]\"\n\ninputs = processor(text, voice_preset=\"v2/de_speaker_1\")\naudio = model.generate(**inputs.to(\"cuda\"))\n\n# Speichern\nwav.write(\"bark_output.wav\", rate=24000, data=audio.cpu().numpy().squeeze())",
      "install_cmd": "pip install transformers scipy"
    },
    {
      "id": "facebook/mms-tts-deu",
      "name": "MMS TTS Deutsch",
      "category": "tts",
      "params": "300M",
      "description": "Metas Massively Multilingual Speech TTS. Leichtgewichtig und schnell.",
      "use_cases": ["German TTS", "Lightweight TTS", "Batch Processing"],
      "recommended": false,
      "python_package": "transformers",
      "python_code": "from transformers import VitsModel, AutoTokenizer\nimport scipy.io.wavfile as wav\nimport torch\n\nmodel = VitsModel.from_pretrained(\"facebook/mms-tts-deu\")\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/mms-tts-deu\")\n\ntext = \"Guten Tag, wie geht es Ihnen heute?\"\ninputs = tokenizer(text, return_tensors=\"pt\")\n\nwith torch.no_grad():\n    output = model(**inputs).waveform\n\nwav.write(\"mms_output.wav\", rate=model.config.sampling_rate, data=output.numpy().squeeze())",
      "install_cmd": "pip install transformers scipy"
    },
    {
      "id": "openai/whisper-large-v3",
      "name": "Whisper Large v3",
      "category": "stt",
      "params": "1.5B",
      "description": "OpenAIs bestes STT-Modell. Unterstützt 99 Sprachen mit hoher Genauigkeit.",
      "use_cases": ["Speech-to-Text", "Transcription", "Translation"],
      "recommended": true,
      "python_package": "transformers",
      "python_code": "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\nimport torch\n\nmodel_id = \"openai/whisper-large-v3\"\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(model_id, torch_dtype=torch.float16, device_map=\"auto\")\nprocessor = AutoProcessor.from_pretrained(model_id)\n\npipe = pipeline(\n    \"automatic-speech-recognition\",\n    model=model,\n    tokenizer=processor.tokenizer,\n    feature_extractor=processor.feature_extractor,\n    torch_dtype=torch.float16,\n    device_map=\"auto\"\n)\n\n# Transkribieren\nresult = pipe(\"audio.mp3\", return_timestamps=True)\nprint(result[\"text\"])\n\n# Mit Timestamps\nfor chunk in result[\"chunks\"]:\n    print(f\"[{chunk['timestamp']}] {chunk['text']}\")",
      "install_cmd": "pip install transformers torch accelerate"
    },
    {
      "id": "openai/whisper-medium",
      "name": "Whisper Medium",
      "category": "stt",
      "params": "769M",
      "description": "Guter Kompromiss zwischen Geschwindigkeit und Genauigkeit. Für die meisten Anwendungen ausreichend.",
      "use_cases": ["Speech-to-Text", "Real-time Transcription"],
      "recommended": true,
      "python_package": "transformers",
      "python_code": "from transformers import pipeline\nimport torch\n\npipe = pipeline(\n    \"automatic-speech-recognition\",\n    model=\"openai/whisper-medium\",\n    torch_dtype=torch.float16,\n    device=\"cuda\"\n)\n\n# Einfache Transkription\nresult = pipe(\"audio.mp3\")\nprint(result[\"text\"])\n\n# Deutsche Sprache erzwingen\nresult = pipe(\"audio.mp3\", generate_kwargs={\"language\": \"german\"})\nprint(result[\"text\"])",
      "install_cmd": "pip install transformers torch"
    },
    {
      "id": "facebook/mms-1b-all",
      "name": "MMS 1B All Languages",
      "category": "stt",
      "params": "1B",
      "description": "Metas Multilingual STT. Unterstützt über 1000 Sprachen!",
      "use_cases": ["Multilingual STT", "Low-resource Languages"],
      "recommended": false,
      "python_package": "transformers",
      "python_code": "from transformers import Wav2Vec2ForCTC, AutoProcessor\nimport torch\nimport librosa\n\nmodel_id = \"facebook/mms-1b-all\"\nprocessor = AutoProcessor.from_pretrained(model_id)\nmodel = Wav2Vec2ForCTC.from_pretrained(model_id)\n\n# Sprache setzen (z.B. Deutsch)\nprocessor.tokenizer.set_target_lang(\"deu\")\nmodel.load_adapter(\"deu\")\n\n# Audio laden\naudio, sr = librosa.load(\"audio.wav\", sr=16000)\ninputs = processor(audio, sampling_rate=16000, return_tensors=\"pt\")\n\nwith torch.no_grad():\n    outputs = model(**inputs).logits\n\nids = torch.argmax(outputs, dim=-1)[0]\ntext = processor.decode(ids)\nprint(text)",
      "install_cmd": "pip install transformers torch librosa"
    }
  ]
}
