# Huggingface Model Finder

Eine Webanwendung zum Entdecken und Vergleichen von Huggingface-Modellen fÃ¼r verschiedene AI-Anwendungen.

## Features

- **Kategorien**: LLM, Embedding (RAG), OCR, TTS, STT
- **Zwei Datenquellen**:
  - â­ **Empfohlen**: Kuratierte Modelle mit Python-Codebeispielen
  - ğŸ”¥ **Trending**: Live von Huggingface API (nach Likes sortiert)
  - ğŸ†• **Neu**: Neueste Modelle von Huggingface
- **Filter**: Alle Modelle â‰¤ 20B Parameter
- **Dokumentation**: Eingebettete Python-Codebeispiele fÃ¼r empfohlene Modelle
- **Dark Theme**: Augenfreundliches Design

## Lokale Entwicklung

Einfach die `index.html` in einem Browser Ã¶ffnen oder einen lokalen Server starten:

```bash
# Mit Python
python -m http.server 8000

# Mit Node.js
npx serve .
```

Dann Ã¶ffne http://localhost:8000

## Docker Deployment

### Mit Docker Compose (empfohlen)

```bash
# Bauen und starten
docker-compose up -d

# Logs anzeigen
docker-compose logs -f

# Stoppen
docker-compose down
```

Die App ist dann unter http://localhost:8080 erreichbar.

### Mit Docker direkt

```bash
# Image bauen
docker build -t hf-model-finder .

# Container starten
docker run -d -p 8080:80 --name hf-model-finder hf-model-finder
```

## Projektstruktur

```
Huggingface-Trends/
â”œâ”€â”€ index.html          # Hauptseite
â”œâ”€â”€ css/
â”‚   â””â”€â”€ styles.css      # Custom Styles
â”œâ”€â”€ js/
â”‚   â””â”€â”€ app.js          # Anwendungslogik
â”œâ”€â”€ data/
â”‚   â””â”€â”€ models.json     # Kuratierte Modelle
â”œâ”€â”€ Dockerfile          # Docker Image
â”œâ”€â”€ docker-compose.yml  # Docker Compose Config
â”œâ”€â”€ nginx.conf          # Nginx Konfiguration
â””â”€â”€ README.md
```

## Modelle hinzufÃ¼gen

Um eigene Modelle zur kuratierten Liste hinzuzufÃ¼gen, bearbeite `data/models.json`:

```json
{
  "id": "organisation/model-name",
  "name": "Model Display Name",
  "category": "llm|embedding|ocr|tts|stt",
  "params": "7B",
  "description": "Beschreibung des Modells",
  "use_cases": ["Chat", "Coding"],
  "recommended": true,
  "python_package": "transformers",
  "python_code": "# Python Code hier...",
  "install_cmd": "pip install transformers torch"
}
```

## Technologien

- **Frontend**: Vanilla JavaScript, Tailwind CSS (CDN)
- **Daten**: Huggingface API + lokale JSON
- **Deployment**: nginx (Docker)

## API Limits

Die Huggingface API ist ohne Token auf ca. 100 Requests/Stunde limitiert. FÃ¼r intensivere Nutzung kann ein Token in `js/app.js` konfiguriert werden.
